# OpenAI Provider Implementation
# Decision: Implements the core LlmProvider trait for OpenAI API
# Decision: Uses OpenAI's chat completion API with streaming support

[package]
name = "everruns-openai"
version.workspace = true
edition.workspace = true
license.workspace = true
authors.workspace = true
description = "OpenAI provider implementation for Everruns"

[dependencies]
# Core dependencies
anyhow.workspace = true
thiserror.workspace = true
async-trait.workspace = true

# Async runtime
tokio.workspace = true
futures.workspace = true

# HTTP client
reqwest.workspace = true
eventsource-stream.workspace = true

# Serialization
serde.workspace = true
serde_json.workspace = true

# Tracing
tracing.workspace = true

# Internal dependencies
everruns-core = { path = "../everruns-core", features = ["openai"] }
everruns-contracts = { path = "../everruns-contracts" }

[dev-dependencies]
tokio = { workspace = true, features = ["full", "test-util"] }
tracing-subscriber.workspace = true
